\documentclass[12pt]{article}

\include{preamble}

\newtoggle{professormode}
\toggletrue{professormode} %STUDENTS: DELETE or COMMENT this line



\title{MATH 390.03-02 / 650 Spring 2016 Homework \#10}

\author{Professor Adam Kapelner} %STUDENTS: write your name here

\iftoggle{professormode}{
\date{Due 8:30AM, Wednesday, May 25, 2016 \\ (material covered on exam, writeup not required -- for extra credit only) \\ \vspace{0.5cm} \small (this document last updated \today ~at \currenttime)}
}

\renewcommand{\abstractname}{Instructions and Philosophy}

\begin{document}
\maketitle

\iftoggle{professormode}{
\begin{abstract}
The path to success in this class is to do many problems. Unlike other courses, exclusively doing reading(s) will not help. Coming to lecture is akin to watching workout videos; thinking about and solving problems on your own is the actual ``working out.''  Feel free to \qu{work out} with others; \textbf{I want you to work on this in groups.}

Reading is still \textit{required}. For this homework set, read about Gibbs sampling and Metropolis-Hastings sampling. Also read ch17 and the epilogue in McGrayne.

The problems below are color coded: \ingreen{green} problems are considered \textit{easy} and marked \qu{[easy]}; \inorange{yellow} problems are considered \textit{intermediate} and marked \qu{[harder]}, \inred{red} problems are considered \textit{difficult} and marked \qu{[difficult]} and \inpurple{purple} problems are extra credit. The \textit{easy} problems are intended to be ``giveaways'' if you went to class. Do as much as you can of the others; I expect you to at least attempt the \textit{difficult} problems. 

Problems marked \qu{[MA]} are for the masters students only (those enrolled in the 650 course). For those in 390, doing these questions will count as extra credit.

This homework is worth 100 points but the point distribution will not be determined until after the due date. See syllabus for the policy on late homework.

Up to 10 points are given as a bonus if the homework is typed using \LaTeX. Links to instaling \LaTeX~and program for compiling \LaTeX~is found on the syllabus. You are encouraged to use \url{overleaf.com}. If you are handing in homework this way, read the comments in the code; there are two lines to comment out and you should replace my name with yours and write your section. The easiest way to use overleaf is to copy the raw text from hwxx.tex and preamble.tex into two new overleaf tex files with the same name. If you are asked to make drawings, you can take a picture of your handwritten drawing and insert them as figures or leave space using the \qu{$\backslash$vspace} command and draw them in after printing or attach them stapled.

The document is available with spaces for you to write your answers. If not using \LaTeX, print this document and write in your answers. I do not accept homeworks which are \textit{not} on this printout. Keep this first page printed for your records.

\end{abstract}

\thispagestyle{empty}
\vspace{1cm}
NAME: \line(1,0){380}
\clearpage
}

\problem{These are questions about McGrayne's book, chapter 17 and the Epilogue. They are optional.}

\begin{enumerate}

\easysubproblem{What do the computer scientists who adopted Bayesian methods care most about and whose view do they subscribe to? (p233)}\spc{1}

\easysubproblem{How was \qu{Stanley} able to cross the Nevada desert?}\spc{3}

\easysubproblem{What two factors are leading to the \qu{crumbling of the Tower of Babel?}}\spc{3}

\intermediatesubproblem{Does the brain work through iterative Bayesian modeling?}\spc{4}

\easysubproblem{According to Geman, what is the most powerful argument for Bayesian Statistics?}\spc{3}

\end{enumerate}


\problem{These are questions which introduce Gibbs Sampling.}

\begin{enumerate}
\easysubproblem{Outline the systematic sweep Gibbs Sampler algorithm below (in your notes).}\spc{6}

\easysubproblem{Pretend you are estimating $\cprob{\theta_1,~\theta_2}{X}$ and the joint posterior looks like the picture below where the $x$ axis is $\theta_1$ and the $y$ axis is $\theta_2$ and darker colors indicate higher probability. Begin at $\angbracks{\theta_1,\theta_2} = \angbracks{0.5,0.5}$ and simulate 5 iterations of the systematic sweep Gibbs sampling algorithm by drawing new points on the plot (just as we did in class).}


\begin{figure}[htp]
\centering
\includegraphics[width=4in]{contour.png}
\end{figure}

\intermediatesubproblem{We previously have shown that if $X~|~\theta \sim \binomial{n}{\theta}$ and the prior on $\theta \sim \betanot{\alpha}{\beta}$, then $X \sim \betabinomial{n}{\alpha}{\beta}$. Even though we proved this result, pretend like you didn't know it and create a Gibbs sampler which finds $\prob{X}$.} \spc{8}

\easysubproblem{Consider the simple linear regression model under OLS with $n$ data points. Assume the true $\beta_0 = 2$ and $\beta_1 = -2$ and $\sigsq = 1$. Let's sample $n=3$ points. Run the code on lines 1--20 of the code at the link \href{https://github.com/kapelner/QC_Math_390.03-02_Spr_2016/blob/master/homeworks/hw10/normal_ridge_gibbs.R}{here} by copying them from the website and pasting them into an R console. This will plot the 3 data points and provide a best fit line. Do you think the best fit line does a good job?}\spc{1}


\easysubproblem{Consider the model in (d). We impose priors of $\beta_0 \sim \normnot{0}{\sigsq / m}$, $\beta_1 \sim \normnot{0}{\sigsq / m}$ and $\sigsq \sim \invgammanot{\overtwo{n_0}}{\overtwo{n_0 \sigsq_0}}$ which is the model we did in class. Find the kernel of the posterior.}\spc{8}

\easysubproblem{Consider the model in (d). Find the conditional distribution of $\sigsq$. The other two conditionals are filled in below so you don't have to complete the squares:

\beqn
\beta_0~|~\beta_1, \sigsq, \X, \y &\sim& \normnot{\frac{\ybar n - \beta_1 \xbar n}{n + m}}{\frac{\sigsq}{n + m}} \\
\beta_1~|~\beta_0, \sigsq, \X, \y &\sim& \normnot{\frac{\displaystyle \sum x_i y_i - \beta_0 \xbar n}{m + \displaystyle \sum x_i^2}}{\frac{\sigsq}{m + \displaystyle \sum x_i^2}} \\ \spc{2}
\cprob{\sigsq}{\beta_0, \beta_1, \X, \y} &\propto& 
\eeqn} \spc{4}

\easysubproblem{Let's run 10,000 iterations of the Gibbs sampler you created in part (f) by running lines  22--78 of the code at the link \href{https://github.com/kapelner/QC_Math_390.03-02_Spr_2016/blob/master/homeworks/hw10/normal_ridge_gibbs.R}{here}. Explain the three plots that popped up. Where would you say the chains burned in at?}\spc{4}


\easysubproblem{Now let's assess autocorrelation for the Gibbs chains in part (g) by running lines 82--87 of the code at the link \href{https://github.com/kapelner/QC_Math_390.03-02_Spr_2016/blob/master/homeworks/hw10/normal_ridge_gibbs.R}{here}. Explain these three plots? What do we mod our chains by to thin them out so the chains represent independent samples?}\spc{4}


\easysubproblem{Now run lines 99 -- 119 of the code at the link \href{https://github.com/kapelner/QC_Math_390.03-02_Spr_2016/blob/master/homeworks/hw10/normal_ridge_gibbs.R}{here} which will first burn and thin the chains. Explain these three plots. What distributions do these frequency histograms approximate? You must have $\prob{\text{something}}$ in your answer. What are the blue lines? What are the red lines? What are the grey lines? Read the code if you have to for the answers.}\spc{4}

\end{enumerate}

\problem{These are questions which deal with the change point model. We did this in class in Lecture 23.}

\begin{enumerate}

\easysubproblem{Consider the change point Poisson model we looked at in class. We have $m$ exchangeable Poisson r.v.'s with parameter $\lambda_1$ followed by $n-m$ exchangeable Poisson r.v.'s with parameter $\lambda_2$. Both rate parameters and the value of $m$ are unknown so the parameter space is 3-dimensional. Write the likelihood below.}\spc{4}

\easysubproblem{Consider the model in (a) where $\lambda_1 = 2$ and $\lambda_2 = 4$ and $m=10$ and $n=30$. Run the code on lines 1--12 of the code at the link \href{https://github.com/kapelner/QC_Math_390.03-02_Spr_2016/blob/master/homeworks/hw10/poisson_gamma_change_pt.R}{here} by copying them from the website and pasting them into an R console. This will plot a realization of the data with those parameters. Can you identify the change point visually?}\spc{1}

\easysubproblem{Consider the model in (a) but we are blinded to the true values of the parameters given in (b) and we wish to estimate them via a Gibbs sampler. Run the code on lines 19--77 of the code at the link \href{https://github.com/kapelner/QC_Math_390.03-02_Spr_2016/blob/master/homeworks/hw10/poisson_gamma_change_pt.R}{here} which will run 10,000 iterations. What iteration number do you think the sampler converged?}\spc{1}

\easysubproblem{Now we wish to assess autocorrelation among the chains from the Gibbs sampler run in (d). Run the code on lines 81--86 of the code at the link \href{https://github.com/kapelner/QC_Math_390.03-02_Spr_2016/blob/master/homeworks/hw10/poisson_gamma_change_pt.R}{here}. What do we mod our chains by to thin them out so the chains represent independent samples?}\spc{1}

\easysubproblem{Run the code on lines 98--118 of the code at the link \href{https://github.com/kapelner/QC_Math_390.03-02_Spr_2016/blob/master/homeworks/hw10/poisson_gamma_change_pt.R}{here} which will first burn and thin the chains. Explain these three plots. What distributions do these frequency histograms approximate? You must have $\prob{\text{something}}$ in your answer. What are the blue lines? What are the red lines? What are the grey lines? Read the code if you have to for the answers.}\spc{4}

\hardsubproblem{Test the following hypothesis: $H_0: m \leq 15$ by approximating the $p$-value from one of the plots in (e).}\spc{4}

\hardsubproblem{[M.A.] Explain a procedure to test $H_0: \lambda_1 = \lambda_2$. You can use the plots if you wish, but you do not have to.}\spc{8}

\hardsubproblem{What exactly would come from $\cprob{X^*}{X}$ in the context of this problem? Assume $X^*$ is the same dimension of $X$ (in our toy example, $n=30$). Explain in full detail. Be careful!}\spc{4}

\hardsubproblem{Explain how you would estimate $\cov{\lambda_1}{\lambda_2}$ and what do you think this estimate will be close to?}\spc{4}


\end{enumerate}


\problem{These are questions which deal with using the Metropolis-Hastings (M-H) algorithm to estimate the parameters in a Poisson regression. We did this in class in Lecture 23.}

\begin{enumerate}

\easysubproblem{Consider the Poisson regression where we have $Y_i \exchdist \poisson{\beta_0 + \beta_1 t_i}$ with $n$ samples. Each sample has s timestamp and a count i.e. $\angbracks{t_i, y_i}$. Write the likelihood below.}\spc{4}


\easysubproblem{Explain why you cannot create a systematic sweep Gibbs sampler for $\cprob{\beta_0, \beta_1}{X}$.}\spc{4}

\easysubproblem{If you cannot use a Gibbs sampler for $\cprob{\beta_0, \beta_1}{X}$, what algorithm can you use instead?}\spc{4}

\easysubproblem{Consider the model in (a) where $\beta_0 = 1$ and $\beta_1 = 3$ and $n=50$. Run the code on lines 1--22 of the code at the link \href{https://github.com/kapelner/QC_Math_390.03-02_Spr_2016/blob/master/homeworks/hw10/poisson_regression_mh.R}{here} by copying the lines (exactly) from the website and pasting them into an R console. This will plot $n$ realizations of the data with $t_i$ evenly spaced in $\bracks{0,2}$. Can you identify the expectation's linear relationship with time visually?}\spc{1}

\easysubproblem{Consider the model in (a) but we are blinded to the true values of the parameters given in (c) and we wish to estimate them via a M-H sampler. Run the code on lines 23--83 of the code at the link \href{https://github.com/kapelner/QC_Math_390.03-02_Spr_2016/blob/master/homeworks/hw10/poisson_regression_mh.R}{here} which will run 100,000 iterations. What iteration number do you think the sampler converged?}\spc{1}

\easysubproblem{Now we wish to assess autocorrelation among the chains from the Gibbs sampler run in (d). Run the code on lines 87--91 of the code at the link \href{https://github.com/kapelner/QC_Math_390.03-02_Spr_2016/blob/master/homeworks/hw10/poisson_regression_mh.R}{here}. What do we mod our chains by to thin them out so the chains represent independent samples?}\spc{1}

\hardsubproblem{Why is the autocorrelation so much higher than what you found in the normal-ridge with unknow variance model's Gibbs sampler and the Poisson change point model's Gibbs sampler (problems 2h and 3d)?}\spc{4}

\easysubproblem{Run the code on lines 100--115 of the code at the link \href{https://github.com/kapelner/QC_Math_390.03-02_Spr_2016/blob/master/homeworks/hw10/ppoisson_regression_mh.R}{here} which will first burn and thin the chains. Explain these three plots. What distributions do these frequency histograms approximate? You must have $\prob{\text{something}}$ in your answer. What are the blue lines? What are the red lines? What are the grey lines? Read the code if you have to for the answers.}\spc{4}

\easysubproblem{Did the M-H sampler \qu{capture} the true parameter values in the approximate 95\% credible region?}\spc{4}

\hardsubproblem{Look at lines 41 and 53 in the code \href{https://github.com/kapelner/QC_Math_390.03-02_Spr_2016/blob/master/homeworks/hw10/ppoisson_regression_mh.R}{here}. What is the M-H \qu{candidate density?} You need to reference line 2 as well.}\spc{4}



\end{enumerate}

\end{document}

